{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdde269e",
   "metadata": {},
   "source": [
    "# Inside Airbnb Madrid: Neighbourhoods & Reviews Integration\n",
    "\n",
    "Objective: Analyze `neighbourhoods.csv` and `reviews.csv`, clean them, and integrate with `calendar_clean` to create enriched datasets with review-derived features and spatial/administrative context.\n",
    "\n",
    "Tasks:\n",
    "- Task A: EDA on neighbourhoods and reviews (data quality, structure, join keys)\n",
    "- Task B: Clean and standardize both datasets\n",
    "- Task C: Integrate reviews (review-derived features) and neighbourhoods with calendar_clean\n",
    "- Deliverables: Cleaned datasets, enriched calendar, integration strategy, web-map suggestions\n",
    "\n",
    "---\n",
    "\n",
    "## Section 0: Environment Setup & Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abad0ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LIBRARY VERSIONS & ENVIRONMENT\n",
      "================================================================================\n",
      "Python: 3.12.12 | packaged by conda-forge | (main, Jan 27 2026, 00:01:15) [Clang 19.1.7 ]\n",
      "pandas: 3.0.0\n",
      "numpy: 2.4.2\n",
      "\n",
      "Project Root: /Users/virginiadimauro/Desktop/UNITN/Secondo Anno/Geospatial Analysis/geospatial-project\n",
      "Dataset Dir: /Users/virginiadimauro/Desktop/UNITN/Secondo Anno/Geospatial Analysis/geospatial-project/data\n",
      "Output Dir: /Users/virginiadimauro/Desktop/UNITN/Secondo Anno/Geospatial Analysis/geospatial-project/data/processed\n",
      "(All paths relative to current working directory)\n",
      "\n",
      "‚úì Environment initialized\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print library versions\n",
    "print(\"=\" * 80)\n",
    "print(\"LIBRARY VERSIONS & ENVIRONMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\\n\")\n",
    "\n",
    "# Configuration - RELATIVE PATHS (reproducible across machines)\n",
    "# Assumes execution from project root directory OR from scripts/ subdirectory\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'scripts' else Path.cwd()\n",
    "DATASET_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = DATASET_DIR / \"processed\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Dataset Dir: {DATASET_DIR}\")\n",
    "print(f\"Output Dir: {OUTPUT_DIR}\")\n",
    "print(f\"(All paths relative to current working directory)\\n\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(\"‚úì Environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15b21b",
   "metadata": {},
   "source": [
    "## Task A1: Neighbourhoods.csv EDA\n",
    "\n",
    "Load and analyze the neighbourhoods dataset: structure, columns, data types, missing values, duplicates, and join potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7033c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NEIGHBOURHOODS.CSV - EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìÅ File: neighbourhoods.csv\n",
      "   Size: 0.00 MB\n",
      "\n",
      "SHAPE & COLUMNS:\n",
      "  Shape: (128, 2) (rows, columns)\n",
      "  Columns: ['neighbourhood_group', 'neighbourhood']\n",
      "\n",
      "DATA TYPES:\n",
      "neighbourhood_group    str\n",
      "neighbourhood          str\n",
      "dtype: object\n",
      "\n",
      "HEAD (first 10 rows):\n",
      "  neighbourhood_group               neighbourhood\n",
      "0          Arganzuela                     Acacias\n",
      "1          Arganzuela                      Atocha\n",
      "2          Arganzuela                     Chopera\n",
      "3          Arganzuela                    Delicias\n",
      "4          Arganzuela                    Imperial\n",
      "5          Arganzuela                     Legazpi\n",
      "6          Arganzuela             Palos de Moguer\n",
      "7             Barajas                  Aeropuerto\n",
      "8             Barajas            Alameda de Osuna\n",
      "9             Barajas  Casco Hist√≥rico de Barajas\n",
      "\n",
      "TAIL (last 5 rows):\n",
      "    neighbourhood_group  neighbourhood\n",
      "123          Villaverde       Butarque\n",
      "124          Villaverde    Los Angeles\n",
      "125          Villaverde    Los Rosales\n",
      "126          Villaverde     San Andr√©s\n",
      "127          Villaverde  San Cristobal\n",
      "\n",
      "MISSING VALUES (per column):\n",
      "  neighbourhood_group: 0 / 128 (0.00%)\n",
      "  neighbourhood: 0 / 128 (0.00%)\n",
      "\n",
      "DUPLICATES ANALYSIS:\n",
      "  Full-row duplicates: 0\n",
      "  Duplicates in 'neighbourhood_group': 107\n",
      "\n",
      "PRIMARY KEY CANDIDATES:\n",
      "  neighbourhood_group: 21 unique values, 0 nulls ‚Üí PK candidate: False\n",
      "  neighbourhood: 128 unique values, 0 nulls ‚Üí PK candidate: True\n",
      "\n",
      "DATA QUALITY CHECKS:\n",
      "\n",
      "‚úì Neighbourhoods EDA complete\n"
     ]
    }
   ],
   "source": [
    "# Load neighbourhoods.csv\n",
    "neighbourhoods_file = DATASET_DIR / \"neighbourhoods.csv\"\n",
    "print(\"=\" * 80)\n",
    "print(\"NEIGHBOURHOODS.CSV - EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìÅ File: {neighbourhoods_file.name}\")\n",
    "print(f\"   Size: {neighbourhoods_file.stat().st_size / (1024**2):.2f} MB\\n\")\n",
    "\n",
    "df_neighbourhoods = pd.read_csv(neighbourhoods_file)\n",
    "\n",
    "print(\"SHAPE & COLUMNS:\")\n",
    "print(f\"  Shape: {df_neighbourhoods.shape} (rows, columns)\")\n",
    "print(f\"  Columns: {list(df_neighbourhoods.columns)}\\n\")\n",
    "\n",
    "print(\"DATA TYPES:\")\n",
    "print(df_neighbourhoods.dtypes)\n",
    "print()\n",
    "\n",
    "print(\"HEAD (first 10 rows):\")\n",
    "print(df_neighbourhoods.head(10))\n",
    "print()\n",
    "\n",
    "print(\"TAIL (last 5 rows):\")\n",
    "print(df_neighbourhoods.tail(5))\n",
    "print()\n",
    "\n",
    "# Missing value analysis\n",
    "print(\"MISSING VALUES (per column):\")\n",
    "missing_pct = (df_neighbourhoods.isnull().sum() / len(df_neighbourhoods) * 100)\n",
    "for col, pct in missing_pct.items():\n",
    "    print(f\"  {col}: {df_neighbourhoods[col].isnull().sum():,} / {len(df_neighbourhoods):,} ({pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Duplicates analysis\n",
    "print(\"DUPLICATES ANALYSIS:\")\n",
    "full_row_dups = df_neighbourhoods.duplicated().sum()\n",
    "print(f\"  Full-row duplicates: {full_row_dups}\")\n",
    "\n",
    "if df_neighbourhoods.shape[1] > 0:\n",
    "    for col in df_neighbourhoods.columns:\n",
    "        col_dups = df_neighbourhoods[col].duplicated().sum()\n",
    "        if col_dups > 0:\n",
    "            print(f\"  Duplicates in '{col}': {col_dups}\")\n",
    "print()\n",
    "\n",
    "# Identify potential primary keys\n",
    "print(\"PRIMARY KEY CANDIDATES:\")\n",
    "for col in df_neighbourhoods.columns:\n",
    "    unique_count = df_neighbourhoods[col].nunique()\n",
    "    is_pk_candidate = (unique_count == len(df_neighbourhoods)) and (df_neighbourhoods[col].isnull().sum() == 0)\n",
    "    print(f\"  {col}: {unique_count} unique values, {df_neighbourhoods[col].isnull().sum()} nulls ‚Üí PK candidate: {is_pk_candidate}\")\n",
    "print()\n",
    "\n",
    "# Data quality issues\n",
    "print(\"DATA QUALITY CHECKS:\")\n",
    "for col in df_neighbourhoods.columns:\n",
    "    if df_neighbourhoods[col].dtype == 'object':\n",
    "        empty_str = (df_neighbourhoods[col] == '').sum()\n",
    "        if empty_str > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  '{col}': {empty_str} empty strings\")\n",
    "        \n",
    "        # Check for leading/trailing spaces\n",
    "        has_spaces = df_neighbourhoods[col].str.strip().ne(df_neighbourhoods[col]).sum()\n",
    "        if has_spaces > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  '{col}': {has_spaces} values with leading/trailing spaces\")\n",
    "\n",
    "print(\"\\n‚úì Neighbourhoods EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044f60e",
   "metadata": {},
   "source": [
    "## Task A2: Reviews.csv EDA\n",
    "\n",
    "Load and analyze the reviews dataset: structure, columns, data types, missing values, review date range, and feasibility for creating aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ae3fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REVIEWS.CSV - EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìÅ File: reviews.csv\n",
      "   Size: 336.53 MB\n",
      "\n",
      "SHAPE & COLUMNS:\n",
      "  Shape: (1275992, 6) (rows, columns)\n",
      "  Columns: ['listing_id', 'id', 'date', 'reviewer_id', 'reviewer_name', 'comments']\n",
      "\n",
      "DATA TYPES:\n",
      "listing_id        Int64\n",
      "id                Int64\n",
      "date             string\n",
      "reviewer_id       Int64\n",
      "reviewer_name    string\n",
      "comments         string\n",
      "dtype: object\n",
      "\n",
      "HEAD (first 10 rows):\n",
      "   listing_id       id        date  reviewer_id reviewer_name  \\\n",
      "0      354838  1100092  2012-04-08       926074      Masahiko   \n",
      "1      354838  2062149  2012-08-22      1915019     Jean-Marc   \n",
      "2      354838  2333538  2012-09-18      2880602     Ekaterina   \n",
      "3      354838  2412223  2012-09-25       751843         Ahmet   \n",
      "4      354838  2466170  2012-10-01      3214589           Ani   \n",
      "5      354838  2642713  2012-10-17      1741900        Andres   \n",
      "6      354838  2927060  2012-11-20      3693269         Ayala   \n",
      "7      354838  3185640  2012-12-30      1649127        Joseph   \n",
      "8      354838  4409814  2013-05-03      5377200       Mikhail   \n",
      "9      354838  4599249  2013-05-14      5816372        Arlete   \n",
      "\n",
      "                                                                                              comments  \n",
      "0  Juan Pedro was a nice host. JP provided detailed tips on what to see around Madrid, with how to ...  \n",
      "1  Very clean and neat appartment. I would recommend anyone to stay at Juanpe's.\\r<br/>Very good co...  \n",
      "2  Juanpe is open, honest and absolutely reliable guy. He really took care about us, even gave us a...  \n",
      "3  The place is nice and in a great location. We stayed for one night and the host was extremely he...  \n",
      "4  Juanpe is amazing person, he left all info regarding touristic attractions (maps,info etc), whic...  \n",
      "5  Juan Pedro is a very kind person. The apartment is very comfortable and easy access from the sub...  \n",
      "6  Great apartment and a great host! The apartment is in a very central location; it's very comfort...  \n",
      "7  Juanpe‚Äôs very nice apartment is one of the many reasons that we enjoyed our stay in Madrid.\\r<br...  \n",
      "8  Juanpe is a very good host, he helped me a lot even before my arrival to Madrid. His appartment ...  \n",
      "9  Apartamento super confort√°vel, limpo, lindo, grande,√≥tima localiza√ß√£o perto de tudo o que precis...  \n",
      "\n",
      "TAIL (last 5 rows):\n",
      "                  listing_id                   id        date  reviewer_id  \\\n",
      "1275987  1477618173421216086  1487624970914547267  2025-08-14    401797295   \n",
      "1275988  1477618173421216086  1493458019493134812  2025-08-22    522550369   \n",
      "1275989  1477618173421216086  1494869191583980239  2025-08-24    681938832   \n",
      "1275990  1477618173421216086  1495617573207593376  2025-08-25    474146370   \n",
      "1275991  1477618173421216086  1500013962055398397  2025-08-31    407567486   \n",
      "\n",
      "          reviewer_name  \\\n",
      "1275987          Zarela   \n",
      "1275988    Julio Isa√≠as   \n",
      "1275989  Carmen Dolores   \n",
      "1275990       Steffanie   \n",
      "1275991       Christian   \n",
      "\n",
      "                                                                                                    comments  \n",
      "1275987                                                                                  Excelente anfitri√≥n  \n",
      "1275988  Departamento muy c√≥modo y bonito, a unos 20min del centro en bus, con muchos servicios alrededor...  \n",
      "1275989  Un sitio muy limpio y en el que se est√° tranquilo, el precio est√° demasiado bien para lo que hay...  \n",
      "1275990  buena ubicaci√≥n, el lugar est√° c√≥modo y lindo lo √∫nico que recomendar√≠a es una cobijas un poco m...  \n",
      "1275991  El lugar estubo bonito, con mucho espacio, todo incluido lo que menciona en el anuncio, el host ...  \n",
      "\n",
      "MISSING VALUES (per column):\n",
      "  listing_id: 0 / 1,275,992 (0.00%)\n",
      "  id: 0 / 1,275,992 (0.00%)\n",
      "  date: 0 / 1,275,992 (0.00%)\n",
      "  reviewer_id: 0 / 1,275,992 (0.00%)\n",
      "  reviewer_name: 3 / 1,275,992 (0.00%)\n",
      "  comments: 104 / 1,275,992 (0.01%)\n",
      "\n",
      "DUPLICATES ANALYSIS:\n",
      "  Full-row duplicates: 0\n",
      "  Duplicates in 'id': 0\n",
      "\n",
      "LISTING_ID ANALYSIS:\n",
      "  Unique listing_ids: 19,853\n",
      "  Null listing_ids: 0\n",
      "\n",
      "REVIEW DATE ANALYSIS:\n",
      "  date:\n",
      "    Range: 2010-07-06 00:00:00 to 2025-09-14 00:00:00\n",
      "    Null values: 0\n",
      "\n",
      "‚úì Reviews EDA complete\n"
     ]
    }
   ],
   "source": [
    "# Load reviews.csv\n",
    "reviews_file = DATASET_DIR / \"reviews.csv\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REVIEWS.CSV - EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìÅ File: {reviews_file.name}\")\n",
    "print(f\"   Size: {reviews_file.stat().st_size / (1024**2):.2f} MB\\n\")\n",
    "\n",
    "# Load with string dtype for date to preserve format\n",
    "df_reviews = pd.read_csv(reviews_file, dtype_backend='numpy_nullable')\n",
    "\n",
    "print(\"SHAPE & COLUMNS:\")\n",
    "print(f\"  Shape: {df_reviews.shape} (rows, columns)\")\n",
    "print(f\"  Columns: {list(df_reviews.columns)}\\n\")\n",
    "\n",
    "print(\"DATA TYPES:\")\n",
    "print(df_reviews.dtypes)\n",
    "print()\n",
    "\n",
    "print(\"HEAD (first 10 rows):\")\n",
    "print(df_reviews.head(10))\n",
    "print()\n",
    "\n",
    "print(\"TAIL (last 5 rows):\")\n",
    "print(df_reviews.tail(5))\n",
    "print()\n",
    "\n",
    "# Missing value analysis\n",
    "print(\"MISSING VALUES (per column):\")\n",
    "missing_pct = (df_reviews.isnull().sum() / len(df_reviews) * 100)\n",
    "for col, pct in missing_pct.items():\n",
    "    print(f\"  {col}: {df_reviews[col].isnull().sum():,} / {len(df_reviews):,} ({pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Duplicates analysis\n",
    "print(\"DUPLICATES ANALYSIS:\")\n",
    "full_row_dups = df_reviews.duplicated().sum()\n",
    "print(f\"  Full-row duplicates: {full_row_dups}\")\n",
    "\n",
    "# Check review_id (likely primary key)\n",
    "if 'id' in df_reviews.columns:\n",
    "    id_dups = df_reviews['id'].duplicated().sum()\n",
    "    print(f\"  Duplicates in 'id': {id_dups}\")\n",
    "print()\n",
    "\n",
    "# Check for listing_id\n",
    "if 'listing_id' in df_reviews.columns:\n",
    "    print(\"LISTING_ID ANALYSIS:\")\n",
    "    print(f\"  Unique listing_ids: {df_reviews['listing_id'].nunique():,}\")\n",
    "    print(f\"  Null listing_ids: {df_reviews['listing_id'].isnull().sum():,}\")\n",
    "print()\n",
    "\n",
    "# Review date analysis\n",
    "print(\"REVIEW DATE ANALYSIS:\")\n",
    "date_cols = [col for col in df_reviews.columns if 'date' in col.lower() or col in ['Date', 'review_date']]\n",
    "if date_cols:\n",
    "    for date_col in date_cols:\n",
    "        try:\n",
    "            df_reviews[date_col] = pd.to_datetime(df_reviews[date_col], errors='coerce')\n",
    "            print(f\"  {date_col}:\")\n",
    "            print(f\"    Range: {df_reviews[date_col].min()} to {df_reviews[date_col].max()}\")\n",
    "            print(f\"    Null values: {df_reviews[date_col].isnull().sum()}\")\n",
    "        except:\n",
    "            print(f\"  {date_col}: Could not parse as datetime\")\n",
    "else:\n",
    "    print(\"  No date columns detected\")\n",
    "print()\n",
    "\n",
    "print(\"‚úì Reviews EDA complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807100f",
   "metadata": {},
   "source": [
    "## Task B1: Neighbourhoods Data Cleaning & Standardization\n",
    "\n",
    "Normalize column names, trim strings, ensure consistent types, verify uniqueness, and save cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "293d7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEIGHBOURHOODS DATA CLEANING\n",
      "================================================================================\n",
      "\n",
      "‚úì Column names normalized: ['neighbourhood_group', 'neighbourhood']\n",
      "\n",
      "Primary Key Verification:\n",
      "  ‚úì 'neighbourhood' is a valid primary key (128 unique, 0 nulls)\n",
      "\n",
      "Final Quality Checks:\n",
      "  ‚úì No full-row duplicates\n",
      "  ‚úì Null values acceptable: 0\n",
      "\n",
      "‚úì Cleaned neighbourhoods saved to: neighbourhoods_clean.parquet\n",
      "  Shape: (128, 2)\n",
      "  Size: 0.00 MB\n",
      "\n",
      "Cleaned neighbourhoods:\n",
      "    neighbourhood_group  neighbourhood\n",
      "0            Arganzuela        Acacias\n",
      "1            Arganzuela         Atocha\n",
      "2            Arganzuela        Chopera\n",
      "3            Arganzuela       Delicias\n",
      "4            Arganzuela       Imperial\n",
      "..                  ...            ...\n",
      "123          Villaverde       Butarque\n",
      "124          Villaverde    Los Angeles\n",
      "125          Villaverde    Los Rosales\n",
      "126          Villaverde     San Andr√©s\n",
      "127          Villaverde  San Cristobal\n",
      "\n",
      "[128 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEIGHBOURHOODS DATA CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_neigh_clean = df_neighbourhoods.copy()\n",
    "\n",
    "# 1. Normalize column names to snake_case\n",
    "df_neigh_clean.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_neigh_clean.columns]\n",
    "print(f\"\\n‚úì Column names normalized: {list(df_neigh_clean.columns)}\")\n",
    "\n",
    "# 2. Trim string values and handle empty strings\n",
    "for col in df_neigh_clean.columns:\n",
    "    if df_neigh_clean[col].dtype == 'object':\n",
    "        df_neigh_clean[col] = df_neigh_clean[col].str.strip()\n",
    "        df_neigh_clean[col] = df_neigh_clean[col].replace('', pd.NA)\n",
    "        print(f\"  Trimmed strings in '{col}'\")\n",
    "\n",
    "# 3. Ensure consistent data types (IDs as int64 if numeric)\n",
    "for col in df_neigh_clean.columns:\n",
    "    if 'id' in col.lower():\n",
    "        try:\n",
    "            df_neigh_clean[col] = pd.to_numeric(df_neigh_clean[col], errors='coerce').astype('Int64')\n",
    "            print(f\"  Converted '{col}' to Int64\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 4. Check for primary key uniqueness\n",
    "print(\"\\nPrimary Key Verification:\")\n",
    "for col in df_neigh_clean.columns:\n",
    "    unique_count = df_neigh_clean[col].nunique()\n",
    "    null_count = df_neigh_clean[col].isnull().sum()\n",
    "    is_pk = (unique_count == len(df_neigh_clean)) and (null_count == 0)\n",
    "    if is_pk:\n",
    "        print(f\"  ‚úì '{col}' is a valid primary key ({unique_count} unique, {null_count} nulls)\")\n",
    "\n",
    "# 5. Final quality checks\n",
    "print(\"\\nFinal Quality Checks:\")\n",
    "assert df_neigh_clean.duplicated().sum() == 0, \"ERROR: Duplicates found after cleaning!\"\n",
    "print(f\"  ‚úì No full-row duplicates\")\n",
    "\n",
    "assert df_neigh_clean.isnull().sum().sum() == 0 or df_neigh_clean.isnull().sum().sum() <= 1, \\\n",
    "    f\"ERROR: More nulls than expected\"\n",
    "print(f\"  ‚úì Null values acceptable: {df_neigh_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# 6. Save cleaned dataset\n",
    "neigh_output_path = OUTPUT_DIR / \"neighbourhoods_clean.parquet\"\n",
    "df_neigh_clean.to_parquet(neigh_output_path, index=False, compression='gzip')\n",
    "print(f\"\\n‚úì Cleaned neighbourhoods saved to: {neigh_output_path.name}\")\n",
    "print(f\"  Shape: {df_neigh_clean.shape}\")\n",
    "print(f\"  Size: {neigh_output_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "print(f\"\\nCleaned neighbourhoods:\")\n",
    "print(df_neigh_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50ca11",
   "metadata": {},
   "source": [
    "## Task B2: Reviews Data Cleaning & Aggregation at Listing Level\n",
    "\n",
    "Clean review records (normalize types, dates), then aggregate by listing_id to create review-derived features for joining with calendar_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eceb56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REVIEWS DATA CLEANING & AGGREGATION\n",
      "================================================================================\n",
      "\n",
      "‚úì Column names normalized\n",
      "  Converted 'date' to datetime64[ns]\n",
      "  Converted 'listing_id' to int64\n",
      "    ‚úì Min value: 21853\n",
      "    ‚úì Max value: 1507409317740478788\n",
      "\n",
      "‚úì Cleaned reviews saved to: reviews_clean.parquet\n",
      "  Shape: (1275992, 6)\n",
      "  Size: 124.54 MB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "AGGREGATING REVIEWS AT LISTING LEVEL\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì Aggregation complete:\n",
      "  Unique listings with reviews: 19853\n",
      "  Columns created: ['listing_id', 'review_count_total', 'last_review_date', 'first_review_date', 'days_since_last_review', 'months_active', 'reviews_per_month', 'reviews_90d']\n",
      "  Sample:\n",
      "   listing_id  review_count_total last_review_date first_review_date  \\\n",
      "0       21853                  33       2018-07-15        2014-10-10   \n",
      "1       30320                 173       2025-08-27        2010-07-06   \n",
      "2       30959                   8       2017-05-30        2015-05-12   \n",
      "3       40916                  53       2025-09-11        2010-11-01   \n",
      "4       62423                 249       2025-09-05        2018-05-10   \n",
      "5       70059                  64       2025-06-23        2011-02-23   \n",
      "6       70073                  36       2025-04-27        2011-10-03   \n",
      "7       70310                  31       2025-05-02        2011-05-03   \n",
      "8       72150                 133       2025-06-08        2011-02-13   \n",
      "9       82481                 105       2017-11-29        2011-05-08   \n",
      "\n",
      "   days_since_last_review  months_active  reviews_per_month  reviews_90d  \n",
      "0                    2767     138.033333           0.239073            0  \n",
      "1                     167     189.933333           0.910846            0  \n",
      "2                    3178     130.900000           0.061115            0  \n",
      "3                     152     186.000000           0.284946            0  \n",
      "4                     158      94.433333           2.636781            0  \n",
      "5                     232     182.200000           0.351262            0  \n",
      "6                     289     174.800000            0.20595            0  \n",
      "7                     284     179.900000           0.172318            0  \n",
      "8                     247     182.533333           0.728634            0  \n",
      "9                    2995     179.733333           0.584199            0  \n",
      "\n",
      "‚úì Aggregated review features saved to: reviews_listing_features.parquet\n",
      "  Shape: (19853, 8)\n",
      "  Size: 0.45 MB\n",
      "\n",
      "‚úì Quality Checks:\n",
      "  ‚úì All review counts > 0\n",
      "  ‚úì No duplicate listing_ids\n",
      "  ‚úì No null listing_ids\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REVIEWS DATA CLEANING & AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_reviews_clean = df_reviews.copy()\n",
    "\n",
    "# 1. Normalize column names to snake_case\n",
    "df_reviews_clean.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_reviews_clean.columns]\n",
    "print(f\"\\n‚úì Column names normalized\")\n",
    "\n",
    "# 2. Trim string values\n",
    "for col in df_reviews_clean.columns:\n",
    "    if df_reviews_clean[col].dtype == 'object':\n",
    "        try:\n",
    "            df_reviews_clean[col] = df_reviews_clean[col].str.strip().replace('', pd.NA)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 3. Convert review date column to datetime64[ns]\n",
    "date_cols = [col for col in df_reviews_clean.columns if 'date' in col.lower()]\n",
    "for date_col in date_cols:\n",
    "    df_reviews_clean[date_col] = pd.to_datetime(df_reviews_clean[date_col], errors='coerce')\n",
    "    print(f\"  Converted '{date_col}' to datetime64[ns]\")\n",
    "\n",
    "# 4. Ensure listing_id is int64 with validation\n",
    "if 'listing_id' in df_reviews_clean.columns:\n",
    "    df_reviews_clean['listing_id'] = df_reviews_clean['listing_id'].astype('int64')\n",
    "    assert df_reviews_clean['listing_id'].min() >= 0, \"ERROR: Found negative listing_id values!\"\n",
    "    assert df_reviews_clean['listing_id'].isnull().sum() == 0, \"ERROR: Found null listing_id values!\"\n",
    "    print(f\"  Converted 'listing_id' to int64\")\n",
    "    print(f\"    ‚úì Min value: {df_reviews_clean['listing_id'].min()}\")\n",
    "    print(f\"    ‚úì Max value: {df_reviews_clean['listing_id'].max()}\")\n",
    "\n",
    "# 5. Save raw cleaned reviews for reference\n",
    "reviews_output_path = OUTPUT_DIR / \"reviews_clean.parquet\"\n",
    "df_reviews_clean.to_parquet(reviews_output_path, index=False, compression='gzip')\n",
    "print(f\"\\n‚úì Cleaned reviews saved to: {reviews_output_path.name}\")\n",
    "print(f\"  Shape: {df_reviews_clean.shape}\")\n",
    "print(f\"  Size: {reviews_output_path.stat().st_size / (1024**2):.2f} MB\\n\")\n",
    "\n",
    "# 6. Aggregate reviews at listing level\n",
    "print(\"-\" * 80)\n",
    "print(\"AGGREGATING REVIEWS AT LISTING LEVEL\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'listing_id' in df_reviews_clean.columns:\n",
    "    # Identify the date column for aggregation\n",
    "    date_col = next((col for col in df_reviews_clean.columns if 'date' in col.lower()), None)\n",
    "    \n",
    "    agg_dict = {\n",
    "        'id': 'count'  # Total count of reviews\n",
    "    }\n",
    "    \n",
    "    # Add date-based aggregations if date column exists\n",
    "    if date_col:\n",
    "        agg_dict[date_col] = ['max', 'min']  # Last review date, first review date\n",
    "    \n",
    "    # Create aggregation\n",
    "    reviews_listing_features = df_reviews_clean.groupby('listing_id').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    if isinstance(reviews_listing_features.columns, pd.MultiIndex):\n",
    "        reviews_listing_features.columns = ['_'.join(col).strip('_') for col in reviews_listing_features.columns]\n",
    "    \n",
    "    # Rename aggregated columns for clarity\n",
    "    reviews_listing_features.rename(columns={'id_count': 'review_count_total'}, inplace=True)\n",
    "    \n",
    "    if date_col:\n",
    "        reviews_listing_features.rename(columns={\n",
    "            f'{date_col}_max': 'last_review_date',\n",
    "            f'{date_col}_min': 'first_review_date'\n",
    "        }, inplace=True)\n",
    "    \n",
    "    # Calculate temporal metrics if date column exists\n",
    "    if date_col and 'last_review_date' in reviews_listing_features.columns:\n",
    "        df_with_dates = reviews_listing_features.dropna(subset=['last_review_date'])\n",
    "        \n",
    "        # Only calculate if there are recent reviews\n",
    "        if len(df_with_dates) > 0:\n",
    "            today = pd.Timestamp.now()\n",
    "            reviews_listing_features['days_since_last_review'] = \\\n",
    "                (today - reviews_listing_features['last_review_date']).dt.days\n",
    "            \n",
    "            # reviews_per_month = reviews_count / months_active\n",
    "            reviews_listing_features['months_active'] = \\\n",
    "                (today - reviews_listing_features['first_review_date']).dt.days / 30.0\n",
    "            \n",
    "            reviews_listing_features['reviews_per_month'] = \\\n",
    "                reviews_listing_features['review_count_total'] / reviews_listing_features['months_active'].clip(lower=1)\n",
    "            \n",
    "            # AGGREGATION METRIC: reviews_90d (recent activity, proxy for demand)\n",
    "            # Count reviews in last 90 days relative to calendar_clean.date.max()\n",
    "            date_90d_ago = today - pd.Timedelta(days=90)\n",
    "            reviews_90d = df_reviews_clean[\n",
    "                (df_reviews_clean['listing_id'].notnull()) & \n",
    "                (df_reviews_clean[date_col] >= date_90d_ago)\n",
    "            ].groupby('listing_id').size().reset_index(name='reviews_90d')\n",
    "            \n",
    "            reviews_listing_features = reviews_listing_features.merge(reviews_90d, on='listing_id', how='left')\n",
    "            reviews_listing_features['reviews_90d'] = reviews_listing_features['reviews_90d'].fillna(0).astype('int64')\n",
    "    \n",
    "    # Ensure int64 for count columns\n",
    "    for col in ['review_count_total', 'reviews_90d']:\n",
    "        if col in reviews_listing_features.columns:\n",
    "            reviews_listing_features[col] = reviews_listing_features[col].astype('int64')\n",
    "    \n",
    "    print(f\"\\n‚úì Aggregation complete:\")\n",
    "    print(f\"  Unique listings with reviews: {len(reviews_listing_features)}\")\n",
    "    print(f\"  Columns created: {list(reviews_listing_features.columns)}\")\n",
    "    print(f\"  Sample:\\n{reviews_listing_features.head(10)}\\n\")\n",
    "    \n",
    "    # Save aggregated features\n",
    "    features_output_path = OUTPUT_DIR / \"reviews_listing_features.parquet\"\n",
    "    reviews_listing_features.to_parquet(features_output_path, index=False, compression='gzip')\n",
    "    print(f\"‚úì Aggregated review features saved to: {features_output_path.name}\")\n",
    "    print(f\"  Shape: {reviews_listing_features.shape}\")\n",
    "    print(f\"  Size: {features_output_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Quality assertions\n",
    "    print(f\"\\n‚úì Quality Checks:\")\n",
    "    assert reviews_listing_features['review_count_total'].min() > 0, \"ERROR: Found zero reviews!\"\n",
    "    assert reviews_listing_features['listing_id'].duplicated().sum() == 0, \"ERROR: Duplicate listing_ids!\"\n",
    "    assert reviews_listing_features['listing_id'].isnull().sum() == 0, \"ERROR: Null listing_ids!\"\n",
    "    print(f\"  ‚úì All review counts > 0\")\n",
    "    print(f\"  ‚úì No duplicate listing_ids\")\n",
    "    print(f\"  ‚úì No null listing_ids\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: 'listing_id' column not found in reviews - skipping aggregation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4ff51",
   "metadata": {},
   "source": [
    "## Task C1: Reviews Integration with Calendar_Clean\n",
    "\n",
    "Load calendar_clean, join review-derived features by listing_id (memory-efficient left join), create calendar_enriched, and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6194b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK C1: REVIEWS INTEGRATION WITH CALENDAR_CLEAN\n",
      "================================================================================\n",
      "\n",
      "üìÅ Loading calendar_clean.csv.gz...\n",
      "  Shape: (9125007, 6)\n",
      "  Columns: ['listing_id', 'date', 'available', 'min_nights', 'max_nights', 'price']\n",
      "  ‚úì listing_id dtype verified: int64\n",
      "\n",
      "üìÅ Loading reviews_listing_features.parquet...\n",
      "  Shape: (19853, 8)\n",
      "  Columns: ['listing_id', 'review_count_total', 'last_review_date', 'first_review_date', 'days_since_last_review', 'months_active', 'reviews_per_month', 'reviews_90d']\n",
      "\n",
      "  Memory usage:\n",
      "    calendar_clean: 365.50 MB\n",
      "    review_features: 1.23 MB\n",
      "\n",
      "  Performing LEFT JOIN on 'listing_id'...\n",
      "  ‚úì Join complete\n",
      "    Result shape: (9125007, 13)\n",
      "\n",
      "  JOIN VALIDATION:\n",
      "  ‚úì Row count preserved: 9125007 == 9125007\n",
      "  ‚úì No duplicates on (listing_id, date): 0\n",
      "  ‚úì Key column types valid (listing_id: int64, date: datetime)\n",
      "  ‚úì Review feature columns created: ['review_count_total', 'last_review_date', 'first_review_date', 'days_since_last_review']\n",
      "      review_count_total: 20.6% null (expected for listings without reviews)\n",
      "      last_review_date: 20.6% null (expected for listings without reviews)\n",
      "      first_review_date: 20.6% null (expected for listings without reviews)\n",
      "      days_since_last_review: 20.6% null (expected for listings without reviews)\n",
      "\n",
      "  DATA TYPES:\n",
      "listing_id                         int64\n",
      "date                      datetime64[us]\n",
      "available                           bool\n",
      "min_nights                         int64\n",
      "max_nights                         int64\n",
      "                               ...      \n",
      "first_review_date         datetime64[us]\n",
      "days_since_last_review           float64\n",
      "months_active                    float64\n",
      "reviews_per_month                Float64\n",
      "reviews_90d                      float64\n",
      "Length: 13, dtype: object\n",
      "\n",
      "‚úì Calendar enriched saved to: calendar_enriched.parquet\n",
      "  Shape: (9125007, 13)\n",
      "  Size: 3.53 MB (-361.96 MB increase)\n",
      "\n",
      "‚úì Task C1 complete: calendar_enriched ready for analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK C1: REVIEWS INTEGRATION WITH CALENDAR_CLEAN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load calendar_clean (try parquet first, then csv.gz)\n",
    "calendar_clean_path = OUTPUT_DIR / \"calendar_clean.parquet\"\n",
    "calendar_clean_csv_path = OUTPUT_DIR / \"calendar_clean.csv.gz\"\n",
    "\n",
    "df_calendar = None\n",
    "\n",
    "if calendar_clean_path.exists():\n",
    "    print(f\"\\nüìÅ Loading {calendar_clean_path.name}...\")\n",
    "    df_calendar = pd.read_parquet(calendar_clean_path)\n",
    "elif calendar_clean_csv_path.exists():\n",
    "    print(f\"\\nüìÅ Loading {calendar_clean_csv_path.name}...\")\n",
    "    df_calendar = pd.read_csv(calendar_clean_csv_path)\n",
    "    # Restore dtypes\n",
    "    df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "    df_calendar['available'] = df_calendar['available'].astype('bool')\n",
    "    df_calendar['listing_id'] = df_calendar['listing_id'].astype('int64')\n",
    "    df_calendar['price'] = df_calendar['price'].astype('Float64')\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: calendar_clean files not found!\")\n",
    "\n",
    "if df_calendar is not None:\n",
    "    print(f\"  Shape: {df_calendar.shape}\")\n",
    "    print(f\"  Columns: {list(df_calendar.columns)}\")\n",
    "    # Type validation\n",
    "    assert df_calendar['listing_id'].dtype == np.int64, f\"ERROR: listing_id dtype is {df_calendar['listing_id'].dtype}, expected int64!\"\n",
    "    print(f\"  ‚úì listing_id dtype verified: int64\")\n",
    "\n",
    "if df_calendar is not None and 'reviews_listing_features' in locals():\n",
    "    print(f\"\\nüìÅ Loading {features_output_path.name}...\")\n",
    "    print(f\"  Shape: {reviews_listing_features.shape}\")\n",
    "    print(f\"  Columns: {list(reviews_listing_features.columns)}\")\n",
    "    \n",
    "    # Memory check before join\n",
    "    mem_cal_mb = df_calendar.memory_usage(deep=True).sum() / (1024**2)\n",
    "    mem_features_mb = reviews_listing_features.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"\\n  Memory usage:\")\n",
    "    print(f\"    calendar_clean: {mem_cal_mb:.2f} MB\")\n",
    "    print(f\"    review_features: {mem_features_mb:.2f} MB\")\n",
    "    \n",
    "    # Left join: calendar_clean on the left, review features on the right\n",
    "    print(f\"\\n  Performing LEFT JOIN on 'listing_id'...\")\n",
    "    df_calendar_enriched = df_calendar.merge(\n",
    "        reviews_listing_features,\n",
    "        on='listing_id',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"  ‚úì Join complete\")\n",
    "    print(f\"    Result shape: {df_calendar_enriched.shape}\")\n",
    "    \n",
    "    # Validation: Check join integrity\n",
    "    print(f\"\\n  JOIN VALIDATION:\")\n",
    "    \n",
    "    # Check 1: Row count preserved (left join semantics)\n",
    "    rows_matched = len(df_calendar_enriched)\n",
    "    rows_original = len(df_calendar)\n",
    "    assert rows_matched == rows_original, f\"ERROR: Row count changed! {rows_original} ‚Üí {rows_matched}\"\n",
    "    print(f\"  ‚úì Row count preserved: {rows_matched} == {rows_original}\")\n",
    "    \n",
    "    # Check 2: No duplicates on (listing_id, date) - CRITICAL\n",
    "    dups = df_calendar_enriched.duplicated(subset=['listing_id', 'date']).sum()\n",
    "    assert dups == 0, f\"ERROR: Found {dups} duplicates on (listing_id, date)!\"\n",
    "    print(f\"  ‚úì No duplicates on (listing_id, date): {dups}\")\n",
    "    \n",
    "    # Check 3: Type validation on key columns\n",
    "    assert df_calendar_enriched['listing_id'].dtype == np.int64, f\"ERROR: listing_id dtype is {df_calendar_enriched['listing_id'].dtype}!\"\n",
    "    assert df_calendar_enriched['date'].dtype.name.startswith('datetime'), f\"ERROR: date dtype is {df_calendar_enriched['date'].dtype}!\"\n",
    "    print(f\"  ‚úì Key column types valid (listing_id: int64, date: datetime)\")\n",
    "    \n",
    "    # Check 4: Review feature columns have expected null patterns\n",
    "    review_feature_cols = [col for col in df_calendar_enriched.columns \n",
    "                           if col.startswith('review_') or col in ['last_review_date', 'first_review_date', 'days_since_last_review']]\n",
    "    if review_feature_cols:\n",
    "        print(f\"  ‚úì Review feature columns created: {review_feature_cols}\")\n",
    "        for col in review_feature_cols:\n",
    "            null_pct = df_calendar_enriched[col].isnull().sum() / len(df_calendar_enriched) * 100\n",
    "            print(f\"      {col}: {null_pct:.1f}% null (expected for listings without reviews)\")\n",
    "    \n",
    "    # Check 5: Data types\n",
    "    print(f\"\\n  DATA TYPES:\")\n",
    "    print(df_calendar_enriched.dtypes)\n",
    "    \n",
    "    # Save enriched calendar\n",
    "    enriched_output_path = OUTPUT_DIR / \"calendar_enriched.parquet\"\n",
    "    df_calendar_enriched.to_parquet(enriched_output_path, index=False, compression='gzip')\n",
    "    enriched_size_mb = enriched_output_path.stat().st_size / (1024**2)\n",
    "    print(f\"\\n‚úì Calendar enriched saved to: {enriched_output_path.name}\")\n",
    "    print(f\"  Shape: {df_calendar_enriched.shape}\")\n",
    "    print(f\"  Size: {enriched_size_mb:.2f} MB ({(enriched_size_mb - mem_cal_mb):.2f} MB increase)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Task C1 complete: calendar_enriched ready for analysis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot proceed with integration: missing calendar_clean or review features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d8151",
   "metadata": {},
   "source": [
    "## Task C2: Neighbourhoods Integration Strategy\n",
    "\n",
    "**IMPORTANT: neighbourhoods.csv contains only METADATA (names/IDs), NOT geometry polygons.**\n",
    "\n",
    "Integration approach: Map via `listings.csv` neighbourhood field using string normalization. Spatial joins with polygon geometry require external GeoJSON/shapefile data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd2f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK C2: NEIGHBOURHOODS INTEGRATION STRATEGY\n",
      "================================================================================\n",
      "\n",
      "üìã NEIGHBOURHOODS ANALYSIS:\n",
      "  Neighbourhoods dataset shape: (128, 2)\n",
      "  Columns: ['neighbourhood_group', 'neighbourhood']\n",
      "  Sample:\n",
      "  neighbourhood_group neighbourhood\n",
      "0          Arganzuela       Acacias\n",
      "1          Arganzuela        Atocha\n",
      "2          Arganzuela       Chopera\n",
      "3          Arganzuela      Delicias\n",
      "4          Arganzuela      Imperial\n",
      "\n",
      "  üìÅ Attempting to load listings.csv to identify neighbourhood join key...\n",
      "    Listings columns: ['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'availability_eoy', 'number_of_reviews_ly', 'estimated_occupancy_l365d', 'estimated_revenue_l365d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'license', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month']\n",
      "    Neighbourhood-related columns in listings: ['host_neighbourhood', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed']\n",
      "\n",
      "  ‚úì Neighbourhood columns found in listings!\n",
      "    Potential join keys: ['host_neighbourhood', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed']\n",
      "    Sample neighbourhood values in listings: <ArrowStringArray>\n",
      "[             'Aluche',                 'Sol',         'Embajadores',\n",
      "            'Justicia',            'Malasa√±a',           'Recoletos',\n",
      " 'Fuencarral-El Pardo',       'Ciudad Lineal',           'La Latina',\n",
      "          'Villaverde']\n",
      "Length: 10, dtype: str\n",
      "\n",
      "  ‚ö†Ô∏è  NEIGHBOURHOODS INTEGRATION STRATEGY:\n",
      "    1. Load full listings.csv with neighbourhood field\n",
      "    2. Create (listing_id, neighbourhood) mapping with string normalization\n",
      "    3. Merge mapping with calendar_enriched on listing_id\n",
      "    4. Optional: merge neighbourhoods metadata on neighbourhood name\n",
      "\n",
      "    NOTE: Attribute matching (not spatial) - string normalization required\n",
      "\n",
      "  Loading full listings.csv for neighbourhood mapping...\n",
      "    Listings shape: (25000, 79)\n",
      "    ‚úì Created mapping: 25,000 listings\n",
      "    Unique neighbourhoods in listings: 149\n",
      "\n",
      "  Joining neighbourhoods with calendar_enriched...\n",
      "    ‚úì Join complete:\n",
      "      Result shape: (9125007, 15)\n",
      "      Null neighbourhoods: 0\n",
      "\n",
      "  Diagnostics: Matching neighbourhoods metadata...\n",
      "\n",
      "‚úì Calendar with neighbourhoods saved to: calendar_enriched_with_neighbourhoods.parquet\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STATUS: NEIGHBOURHOODS.CSV CONTENT ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì CONFIRMED: neighbourhoods.csv contains ATTRIBUTE METADATA ONLY (no geometry):\n",
      "  - Format: Simple CSV table (128 neighbourhoods √ó 2 columns)\n",
      "  - Columns: neighbourhood_group (admin district), neighbourhood (name)\n",
      "  - Primary key: neighbourhood (name)\n",
      "  - Use case: Lookup table for neighbourhood metadata\n",
      "\n",
      "‚ö†Ô∏è  LIMITATION: Cannot enable true spatial analysis without polygon boundaries:\n",
      "  - No direct neighbourhood-level mapping (no geometry)\n",
      "  - Requires intermediate mapping via listings.csv (attribute join)\n",
      "  - String normalization essential (case/whitespace handling)\n",
      "\n",
      "üîÑ INTEGRATION STRATEGY:\n",
      "  1. Create (listing_id ‚Üí neighbourhood) mapping from listings.csv\n",
      "  2. String normalization: .lower().strip() on neighbourhood names\n",
      "  3. Left join mapping with calendar_enriched by listing_id\n",
      "  4. Optional: merge neighbourhoods metadata for additional attributes\n",
      "\n",
      "üó∫Ô∏è  FOR SPATIAL ANALYSIS (if needed):\n",
      "  - Requires external GeoJSON/shapefile with neighbourhood polygons + coordinates\n",
      "  - Then use GeoPandas spatial join (sjoin) on geometry\n",
      "  - Future work: integrate spatial_data.geojson when available\n",
      "\n",
      "üìä CURRENT OUTPUT: calendar_enriched_with_neighbourhoods.parquet\n",
      "  - Contains neighbourhood names via attribute mapping\n",
      "  - Ready for neighbourhood-level aggregations (not spatial operations)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK C2: NEIGHBOURHOODS INTEGRATION STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìã NEIGHBOURHOODS ANALYSIS:\")\n",
    "print(f\"  Neighbourhoods dataset shape: {df_neigh_clean.shape}\")\n",
    "print(f\"  Columns: {list(df_neigh_clean.columns)}\")\n",
    "print(f\"  Sample:\\n{df_neigh_clean.head()}\\n\")\n",
    "\n",
    "# Strategy: Try to join neighbourhoods via listings.csv\n",
    "listings_file = DATASET_DIR / \"listings.csv\"\n",
    "if listings_file.exists():\n",
    "    print(f\"  üìÅ Attempting to load listings.csv to identify neighbourhood join key...\")\n",
    "    \n",
    "    # Read listings with specific columns only (memory efficient)\n",
    "    listings_cols_to_check = ['id', 'listing_id']\n",
    "    listings_cols_to_check.extend([col for col in ['neighbourhood', 'neighbourhood_cleansed', \n",
    "                                                     'neighbourhood_group_cleansed', 'neighbourhood_group'] \n",
    "                                   if col in ['neighbourhood', 'neighbourhood_cleansed', \n",
    "                                             'neighbourhood_group_cleansed', 'neighbourhood_group']])\n",
    "    \n",
    "    try:\n",
    "        # Sample listings to find join key\n",
    "        df_listings_sample = pd.read_csv(listings_file, nrows=100)\n",
    "        listing_cols = list(df_listings_sample.columns)\n",
    "        print(f\"    Listings columns: {listing_cols}\")\n",
    "        \n",
    "        # Identify neighbourhood-related columns\n",
    "        neigh_cols_in_listings = [col for col in listing_cols if 'neighbourhood' in col.lower()]\n",
    "        neigh_names_in_listings = [col for col in listing_cols if 'neigh' in col.lower()]\n",
    "        \n",
    "        print(f\"    Neighbourhood-related columns in listings: {neigh_cols_in_listings}\")\n",
    "        \n",
    "        if neigh_cols_in_listings:\n",
    "            print(f\"\\n  ‚úì Neighbourhood columns found in listings!\")\n",
    "            print(f\"    Potential join keys: {neigh_cols_in_listings}\")\n",
    "            \n",
    "            # Try to find common neighbourhood names between listings and neighbourhoods\n",
    "            neigh_names_listings = df_listings_sample[neigh_cols_in_listings[0]].unique()[:10]\n",
    "            print(f\"    Sample neighbourhood values in listings: {neigh_names_listings}\")\n",
    "            \n",
    "            if len(df_neigh_clean) < 100:\n",
    "                print(f\"    Neighbourhoods dataset content:\")\n",
    "                print(df_neigh_clean)\n",
    "            \n",
    "            print(f\"\\n  ‚ö†Ô∏è  NEIGHBOURHOODS INTEGRATION STRATEGY:\")\n",
    "            print(f\"    1. Load full listings.csv with neighbourhood field\")\n",
    "            print(f\"    2. Create (listing_id, neighbourhood) mapping with string normalization\")\n",
    "            print(f\"    3. Merge mapping with calendar_enriched on listing_id\")\n",
    "            print(f\"    4. Optional: merge neighbourhoods metadata on neighbourhood name\")\n",
    "            print(f\"\\n    NOTE: Attribute matching (not spatial) - string normalization required\")\n",
    "            \n",
    "            # Load full listings for mapping (with memory consideration)\n",
    "            print(f\"\\n  Loading full listings.csv for neighbourhood mapping...\")\n",
    "            df_listings = pd.read_csv(listings_file, dtype={'id': 'int64'})\n",
    "            print(f\"    Listings shape: {df_listings.shape}\")\n",
    "            \n",
    "            neigh_col = neigh_cols_in_listings[0] if neigh_cols_in_listings else None\n",
    "            if neigh_col:\n",
    "                # Create listings-to-neighbourhood mapping\n",
    "                listing_neighbourhood_map = df_listings[['id', neigh_col]].drop_duplicates()\n",
    "                listing_neighbourhood_map.columns = ['listing_id', 'neighbourhood_listings']\n",
    "                listing_neighbourhood_map['listing_id'] = listing_neighbourhood_map['listing_id'].astype('int64')\n",
    "                \n",
    "                # Normalize strings for matching\n",
    "                listing_neighbourhood_map['neighbourhood_normalized'] = (\n",
    "                    listing_neighbourhood_map['neighbourhood_listings']\n",
    "                    .fillna('Unknown')\n",
    "                    .str.lower()\n",
    "                    .str.strip()\n",
    "                )\n",
    "                \n",
    "                print(f\"    ‚úì Created mapping: {listing_neighbourhood_map.shape[0]:,} listings\")\n",
    "                print(f\"    Unique neighbourhoods in listings: {listing_neighbourhood_map['neighbourhood_normalized'].nunique()}\")\n",
    "                \n",
    "                # Try to join with calendar_enriched\n",
    "                if 'df_calendar_enriched' in locals():\n",
    "                    print(f\"\\n  Joining neighbourhoods with calendar_enriched...\")\n",
    "                    df_calendar_with_neigh = df_calendar_enriched.merge(\n",
    "                        listing_neighbourhood_map[['listing_id', 'neighbourhood_normalized', 'neighbourhood_listings']],\n",
    "                        on='listing_id',\n",
    "                        how='left'\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"    ‚úì Join complete:\")\n",
    "                    print(f\"      Result shape: {df_calendar_with_neigh.shape}\")\n",
    "                    print(f\"      Null neighbourhoods: {df_calendar_with_neigh['neighbourhood_normalized'].isnull().sum()}\")\n",
    "                    \n",
    "                    # Try to add neighbourhood metadata\n",
    "                    if len(df_neigh_clean) > 0:\n",
    "                        print(f\"\\n  Diagnostics: Matching neighbourhoods metadata...\")\n",
    "                        \n",
    "                        # Normalize neighbourhoods dataset\n",
    "                        df_neigh_clean_norm = df_neigh_clean.copy()\n",
    "                        if df_neigh_clean_norm.shape[1] > 0:\n",
    "                            text_col = [col for col in df_neigh_clean_norm.columns \n",
    "                                       if df_neigh_clean_norm[col].dtype == 'object'][0] if any(\n",
    "                                           df_neigh_clean_norm[col].dtype == 'object' \n",
    "                                           for col in df_neigh_clean_norm.columns) else None\n",
    "                            \n",
    "                            if text_col:\n",
    "                                df_neigh_clean_norm[f'{text_col}_normalized'] = (\n",
    "                                    df_neigh_clean_norm[text_col].fillna('Unknown')\n",
    "                                    .str.lower()\n",
    "                                    .str.strip()\n",
    "                                )\n",
    "                                \n",
    "                                matches = df_calendar_with_neigh['neighbourhood_normalized'].isin(\n",
    "                                    df_neigh_clean_norm[f'{text_col}_normalized']\n",
    "                                ).sum()\n",
    "                                print(f\"    Matches found: {matches} / {len(df_calendar_with_neigh):,} rows\")\n",
    "                    \n",
    "                    # Save with neighbourhoods\n",
    "                    neigh_enriched_path = OUTPUT_DIR / \"calendar_enriched_with_neighbourhoods.parquet\"\n",
    "                    df_calendar_with_neigh.to_parquet(neigh_enriched_path, index=False, compression='gzip')\n",
    "                    print(f\"\\n‚úì Calendar with neighbourhoods saved to: {neigh_enriched_path.name}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  No clear neighbourhood column in listings\")\n",
    "            print(f\"    Neighbourhoods dataset likely contains only metadata (requires spatial join with GeoJSON/shapefile)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error processing listings: {str(e)[:100]}\")\n",
    "        print(f\"    Neighbourhoods cannot be joined via attribute - spatial join needed (see section below)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  listings.csv not found\")\n",
    "    print(f\"  Neighbourhoods integration requires external listings data or spatial join capability\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 80)\n",
    "print(f\"STATUS: NEIGHBOURHOODS.CSV CONTENT ANALYSIS\")\n",
    "print(f\"-\" * 80)\n",
    "print(f\"\"\"\n",
    "‚úì CONFIRMED: neighbourhoods.csv contains ATTRIBUTE METADATA ONLY (no geometry):\n",
    "  - Format: Simple CSV table (128 neighbourhoods √ó 2 columns)\n",
    "  - Columns: neighbourhood_group (admin district), neighbourhood (name)\n",
    "  - Primary key: neighbourhood (name)\n",
    "  - Use case: Lookup table for neighbourhood metadata\n",
    "\n",
    "‚ö†Ô∏è  LIMITATION: Cannot enable true spatial analysis without polygon boundaries:\n",
    "  - No direct neighbourhood-level mapping (no geometry)\n",
    "  - Requires intermediate mapping via listings.csv (attribute join)\n",
    "  - String normalization essential (case/whitespace handling)\n",
    "\n",
    "üîÑ INTEGRATION STRATEGY:\n",
    "  1. Create (listing_id ‚Üí neighbourhood) mapping from listings.csv\n",
    "  2. String normalization: .lower().strip() on neighbourhood names\n",
    "  3. Left join mapping with calendar_enriched by listing_id\n",
    "  4. Optional: merge neighbourhoods metadata for additional attributes\n",
    "\n",
    "üó∫Ô∏è  FOR SPATIAL ANALYSIS (if needed):\n",
    "  - Requires external GeoJSON/shapefile with neighbourhood polygons + coordinates\n",
    "  - Then use GeoPandas spatial join (sjoin) on geometry\n",
    "  - Future work: integrate spatial_data.geojson when available\n",
    "\n",
    "üìä CURRENT OUTPUT: calendar_enriched_with_neighbourhoods.parquet\n",
    "  - Contains neighbourhood names via attribute mapping\n",
    "  - Ready for neighbourhood-level aggregations (not spatial operations)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93794b8",
   "metadata": {},
   "source": [
    "## Integration Quality Assurance & Final Summary\n",
    "\n",
    "Comprehensive integrity checks on enriched datasets and summary of deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba2f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUALITY ASSURANCE & FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üì¶ CLEANED DATASETS:\n",
      "  ‚úì neighbourhoods_clean.parquet: (128, 2)\n",
      "  ‚úì reviews_clean.parquet: (1275992, 6)\n",
      "  ‚úì reviews_listing_features.parquet: (19853, 8)\n",
      "\n",
      "üíæ ENRICHED DATASETS:\n",
      "  ‚úì calendar_enriched.parquet: (9125007, 13)\n",
      "    Columns added: ['review_count_total', 'last_review_date', 'first_review_date', 'days_since_last_review', 'reviews_per_month', 'reviews_90d']\n",
      "  ‚úì calendar_enriched_with_neighbourhoods.parquet: (9125007, 15)\n",
      "\n",
      "üîí CRITICAL ASSERTIONS:\n",
      "  ‚úì No duplicates on (listing_id, date)\n",
      "  ‚úì listing_id is int64\n",
      "\n",
      "üìã RECOMMENDATIONS:\n",
      "  ‚Ä¢ Reviews: Use review_count_total, reviews_per_month, reviews_90d (last 90 days) as demand/activity proxies\n",
      "  ‚Ä¢ Neighbourhoods: Attribute mapping confirmed; spatial analysis requires external GeoJSON/shapefile\n",
      "  ‚Ä¢ For web-map: Use reviews_90d (recent activity) as marker size/color, availability_rate as heatmap\n",
      "\n",
      "‚úì Quality Assurance complete\n",
      "\n",
      "Full Report:\n",
      "{\n",
      "  \"datasets_processed\": [\n",
      "    [\n",
      "      \"neighbourhoods_clean\",\n",
      "      [\n",
      "        128,\n",
      "        2\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"reviews_clean\",\n",
      "      [\n",
      "        1275992,\n",
      "        6\n",
      "      ]\n",
      "    ]\n",
      "  ],\n",
      "  \"artifacts_created\": [\n",
      "    \"neighbourhoods_clean.parquet\",\n",
      "    \"reviews_clean.parquet\",\n",
      "    \"reviews_listing_features.parquet\",\n",
      "    \"calendar_enriched.parquet\",\n",
      "    \"calendar_enriched_with_neighbourhoods.parquet\"\n",
      "  ],\n",
      "  \"integration_status\": {\n",
      "    \"reviews\": \"\\u2713 Integrated\",\n",
      "    \"neighbourhoods\": \"\\u2713 Integrated via listings mapping\"\n",
      "  },\n",
      "  \"recommendations\": [\n",
      "    \"Reviews: Use review_count_total, reviews_per_month, reviews_90d (last 90 days) as demand/activity proxies\",\n",
      "    \"Neighbourhoods: Attribute mapping confirmed; spatial analysis requires external GeoJSON/shapefile\",\n",
      "    \"For web-map: Use reviews_90d (recent activity) as marker size/color, availability_rate as heatmap\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUALITY ASSURANCE & FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_report = {\n",
    "    \"datasets_processed\": [],\n",
    "    \"artifacts_created\": [],\n",
    "    \"integration_status\": {},\n",
    "    \"recommendations\": []\n",
    "}\n",
    "\n",
    "# Check cleaned datasets\n",
    "print(\"\\nüì¶ CLEANED DATASETS:\")\n",
    "if (OUTPUT_DIR / \"neighbourhoods_clean.parquet\").exists():\n",
    "    df_neigh = pd.read_parquet(OUTPUT_DIR / \"neighbourhoods_clean.parquet\")\n",
    "    print(f\"  ‚úì neighbourhoods_clean.parquet: {df_neigh.shape}\")\n",
    "    summary_report[\"datasets_processed\"].append((\"neighbourhoods_clean\", df_neigh.shape))\n",
    "    summary_report[\"artifacts_created\"].append(\"neighbourhoods_clean.parquet\")\n",
    "\n",
    "if (OUTPUT_DIR / \"reviews_clean.parquet\").exists():\n",
    "    df_rev = pd.read_parquet(OUTPUT_DIR / \"reviews_clean.parquet\")\n",
    "    print(f\"  ‚úì reviews_clean.parquet: {df_rev.shape}\")\n",
    "    summary_report[\"datasets_processed\"].append((\"reviews_clean\", df_rev.shape))\n",
    "    summary_report[\"artifacts_created\"].append(\"reviews_clean.parquet\")\n",
    "\n",
    "if (OUTPUT_DIR / \"reviews_listing_features.parquet\").exists():\n",
    "    df_rev_feat = pd.read_parquet(OUTPUT_DIR / \"reviews_listing_features.parquet\")\n",
    "    print(f\"  ‚úì reviews_listing_features.parquet: {df_rev_feat.shape}\")\n",
    "    summary_report[\"artifacts_created\"].append(\"reviews_listing_features.parquet\")\n",
    "\n",
    "# Check enriched datasets\n",
    "print(\"\\nüíæ ENRICHED DATASETS:\")\n",
    "if (OUTPUT_DIR / \"calendar_enriched.parquet\").exists():\n",
    "    df_cal_enr = pd.read_parquet(OUTPUT_DIR / \"calendar_enriched.parquet\")\n",
    "    print(f\"  ‚úì calendar_enriched.parquet: {df_cal_enr.shape}\")\n",
    "    print(f\"    Columns added: {[col for col in df_cal_enr.columns if 'review' in col.lower() or col in ['last_review_date', 'first_review_date', 'days_since_last_review', 'reviews_per_month', 'reviews_90d']]}\")\n",
    "    summary_report[\"artifacts_created\"].append(\"calendar_enriched.parquet\")\n",
    "    summary_report[\"integration_status\"][\"reviews\"] = \"‚úì Integrated\"\n",
    "\n",
    "if (OUTPUT_DIR / \"calendar_enriched_with_neighbourhoods.parquet\").exists():\n",
    "    df_cal_neigh = pd.read_parquet(OUTPUT_DIR / \"calendar_enriched_with_neighbourhoods.parquet\")\n",
    "    print(f\"  ‚úì calendar_enriched_with_neighbourhoods.parquet: {df_cal_neigh.shape}\")\n",
    "    summary_report[\"artifacts_created\"].append(\"calendar_enriched_with_neighbourhoods.parquet\")\n",
    "    summary_report[\"integration_status\"][\"neighbourhoods\"] = \"‚úì Integrated via listings mapping\"\n",
    "\n",
    "# Final integrity assertions\n",
    "if 'df_calendar_enriched' in locals():\n",
    "    print(f\"\\nüîí CRITICAL ASSERTIONS:\")\n",
    "    assert df_calendar_enriched.duplicated(subset=['listing_id', 'date']).sum() == 0, \\\n",
    "        \"CRITICAL ERROR: Duplicates found on (listing_id, date)!\"\n",
    "    assert df_calendar_enriched['listing_id'].dtype == np.int64, \\\n",
    "        f\"CRITICAL ERROR: listing_id dtype is {df_calendar_enriched['listing_id'].dtype}!\"\n",
    "    print(f\"  ‚úì No duplicates on (listing_id, date)\")\n",
    "    print(f\"  ‚úì listing_id is int64\")\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "summary_report[\"recommendations\"].append(\n",
    "    \"Reviews: Use review_count_total, reviews_per_month, reviews_90d (last 90 days) as demand/activity proxies\"\n",
    ")\n",
    "summary_report[\"recommendations\"].append(\n",
    "    \"Neighbourhoods: Attribute mapping confirmed; spatial analysis requires external GeoJSON/shapefile\"\n",
    ")\n",
    "summary_report[\"recommendations\"].append(\n",
    "    \"For web-map: Use reviews_90d (recent activity) as marker size/color, availability_rate as heatmap\"\n",
    ")\n",
    "\n",
    "print(\"\\n\".join([f\"  ‚Ä¢ {r}\" for r in summary_report[\"recommendations\"]]))\n",
    "\n",
    "print(f\"\\n‚úì Quality Assurance complete\")\n",
    "print(f\"\\nFull Report:\")\n",
    "import json\n",
    "print(json.dumps(summary_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b8c8c",
   "metadata": {},
   "source": [
    "## Integration Summary: Research Value & Join Strategy\n",
    "\n",
    "What each dataset adds:\n",
    "\n",
    "- **neighbourhoods.csv**: Geographic/administrative context (neighbourhood names/IDs). Enables neighbourhood-level analysis (availability by district, review intensity by area).\n",
    "\n",
    "- **reviews.csv**: Demand proxy and listing quality indicator. Aggregated at listing level: review counts, temporal patterns (last review date), and intensity (reviews/month, reviews in last 90 days).\n",
    "\n",
    "Valid joins:\n",
    "\n",
    "1. **Reviews ‚Üí Calendar**: Recommended\n",
    "   - `reviews.listing_id` ‚Üí `calendar.listing_id` (left join)\n",
    "   - Creates review-derived features at each calendar row\n",
    "   - No data loss; listings without reviews get NA values\n",
    "   - Result: `calendar_enriched.parquet` (9.1M rows + review columns)\n",
    "\n",
    "2. **Neighbourhoods ‚Üí Calendar**: Depends on data format\n",
    "   - If neighbourhoods.csv contains attribute data (names/IDs from listings.csv):\n",
    "     - Join via `listings.csv` neighbourhood field (string normalization needed)\n",
    "     - Result: `calendar_enriched_with_neighbourhoods.parquet`\n",
    "   - If neighbourhoods.csv contains polygon geometries (GeoJSON/shapefile):\n",
    "     - Requires spatial join using GeoPandas (not implemented here)\n",
    "     - Needs coordinates from listings.csv (latitude/longitude)\n",
    "\n",
    "Limitations & Assumptions:\n",
    "\n",
    "- **Reviews**: Aggregate per listing loses temporal patterns within a listing's history. Treated as static proxy for demand quality, not dynamic pricing signal.\n",
    "- **Review intensity** (reviews_per_month): Proxy for popularity/demand, not actual booking volume (needs booking data).\n",
    "- **Neighbourhoods without geometry**: Cannot enable true spatial analysis without polygon boundaries (e.g., mapping availability heatmaps by district).\n",
    "- **Last review date**: Useful for identifying inactive listings, but review date distribution may be seasonal/biased.\n",
    "\n",
    "Recommended downstream analyses:\n",
    "\n",
    "1. Availability by review intensity (filter calendars by reviews_per_month quartiles, compare avg_availability)\n",
    "2. Neighbourhood-level review statistics (mean reviews_per_month, review_count_total by district)\n",
    "3. Seasonality patterns filtered by neighbourhood (lag availability by neighbourhood + season)\n",
    "4. Interactive web-map: click neighbourhood ‚Üí see availability + review stats + recent reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec393464",
   "metadata": {},
   "source": [
    "## Interactive Web-Map Suggestions\n",
    "\n",
    "### Concept 1: Review Intensity & Availability Heatmap\n",
    "\n",
    "Goal: Visualize listing availability colored by review intensity, with neighbourhood filtering.\n",
    "\n",
    "Data source: `calendar_enriched.parquet` (aggregated to listing level: mean availability, review_per_month)\n",
    "\n",
    "Interactive elements:\n",
    "- Listing markers: colored by `reviews_per_month` (blue=low activity, red=high activity)\n",
    "- Popup: neighbourhood, price, last_review_date, available_pct (last 30 days)\n",
    "- Filter: dropdown for neighbourhood(s), slider for `review_count_total` (minimum)\n",
    "- Heatmap toggle: switch between (a) review intensity overlay, (b) availability overlay\n",
    "\n",
    "Libraries: Folium (Leaflet wrapper) or Plotly\n",
    "\n",
    "---\n",
    "\n",
    "### Concept 2: Neighbourhood Review & Availability Dashboard\n",
    "\n",
    "Goal: Neighbourhood-level summary (mean review metrics, availability by month, review timeline)\n",
    "\n",
    "Data aggregation (listing ‚Üí neighbourhood ‚Üí calendar row):\n",
    "- Group by neighbourhood and time_window (month/week)\n",
    "- Calculate: avg_availability, median_reviews_per_month, count_of_active_listings\n",
    "- Compute: review_trend (reviews_90d / reviews_total ratio)\n",
    "\n",
    "Interactive elements:\n",
    "- Neighbourhood dropdown: select 1-5 neighbourhoods to compare\n",
    "- Time range slider: filter to specific months\n",
    "- Charts:\n",
    "  - Time series: availability vs review_intensity over time\n",
    "  - Box plot: distribution of reviews_per_month by neighbourhood\n",
    "  - Bar chart: count of active listings per neighbourhood\n",
    "\n",
    "Libraries: Plotly (interactive) + GeoPandas (if geometry available)\n",
    "\n",
    "---\n",
    "\n",
    "### Code Template (Folium Concept 1)\n",
    "\n",
    "```python\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium import plugins\n",
    "\n",
    "# Load calendar_enriched + listings (for coordinates)\n",
    "df_enriched = pd.read_parquet('data/processed/calendar_enriched.parquet')\n",
    "df_listings = pd.read_csv('data/calendar.csv')  # or listings.csv with lat/lon\n",
    "\n",
    "# Aggregate to listing level (mean over dates)\n",
    "listing_stats = df_enriched.groupby('listing_id').agg({\n",
    "    'available': 'mean',\n",
    "    'review_count_total': 'first',\n",
    "    'reviews_per_month': 'first',\n",
    "    'last_review_date': 'first',\n",
    "    'price': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge with coordinates\n",
    "df_map_data = listing_stats.merge(df_listings[['id', 'latitude', 'longitude', 'neighbourhood_cleansed']], \n",
    "                                   left_on='listing_id', right_on='id', how='left')\n",
    "\n",
    "# Color scale: reviews_per_month\n",
    "def color_reviews(val):\n",
    "    if pd.isna(val): return 'gray'\n",
    "    if val < 1: return 'blue'\n",
    "    if val < 2: return 'green'\n",
    "    if val < 3: return 'yellow'\n",
    "    return 'red'\n",
    "\n",
    "# Create map\n",
    "m = folium.Map(location=[40.4168, -3.7038], zoom_start=12)  # Madrid center\n",
    "\n",
    "# Add markers\n",
    "for _, row in df_map_data.iterrows():\n",
    "    if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            popup=f\"{row['neighbourhood_cleansed']}<br>Reviews/month: {row['reviews_per_month']:.2f}<br>Availability: {row['available']:.1%}\",\n",
    "            color=color_reviews(row['reviews_per_month']),\n",
    "            fill=True,\n",
    "            fillOpacity=0.7\n",
    "        ).add_to(m)\n",
    "\n",
    "# Save\n",
    "m.save('data/processed/review_intensity_map.html')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps for Interactive Filtering\n",
    "\n",
    "To add dropdown & slider filtering, wrap map generation in Streamlit or Jupyter widgets:\n",
    "\n",
    "```python\n",
    "# Streamlit example:\n",
    "import streamlit as st\n",
    "\n",
    "selected_neighbourhood = st.selectbox('Neighbourhood', df_map_data['neighbourhood_cleansed'].unique())\n",
    "min_reviews = st.slider('Min reviews/month', 0.0, 10.0, 0.0)\n",
    "\n",
    "filtered_data = df_map_data[\n",
    "    (df_map_data['neighbourhood_cleansed'] == selected_neighbourhood) &\n",
    "    (df_map_data['reviews_per_month'] >= min_reviews)\n",
    "]\n",
    "\n",
    "# Generate map with filtered_data...\n",
    "```\n",
    "\n",
    "Implementation requires coordinate data from listings.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
